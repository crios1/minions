# TODO: write my docs for this project based off of 
# the amazing docs of prompt-toolkit
# https://python-prompt-toolkit.readthedocs.io/en/master/index.html
# also this is interesting but maybe not as good: https://microsoft.github.io/autogen/stable/

# TODO: consider have the package be "pip installable" as "minimal" where the library just ships with the python needed to run it, no other quality of life improvements

# TODO:
ManageRisks:
- OOMs -> gru can reject new start_minion requests ... should it also suspend pipelines (and maybe others)?
- library seg faults -> are rare and infrequent,
                        when they happen everything restarts from systemd
                        and jornald will log the stdout so dev can figure out what segfaulted

- dev's should check the staleness of thier context/event in thier steps cuz workflow may be resumed after a power failure 
  maybe i simplify that as a decorator or something if i see that i keep using the same patern every step



Short answer: yes—**a real SIGSEGV in a C/C++ extension is unrecoverable for that process.** You can’t “catch” it in Python and safely continue. The only reliable mitigation is **process isolation** + **supervision**.

What’s true:

* A segfault (SIGSEGV) or abort (SIGABRT) will terminate the process (often with a core dump). Returning control to Python is undefined/unsafe.
* Python signal handlers/`try/except` can’t save you here. The GIL doesn’t help.
* You *can* log diagnostics right before death.

What to do (practical):

* **Isolate risky bits**: run libraries that touch native code (numba/JIT, pyarrow, exchange SDKs, BLAS stacks, custom C-extensions) in a **sidecar subprocess** with a tight API (Unix socket/msgpack). Let systemd supervise and auto-restart.
* **Add a hard watchdog**: if RSS > limit, event-loop lag > N ms, or health probe fails → terminate and let the supervisor restart.
* **Enable crash forensics** early:

  * `faulthandler.enable()` and `faulthandler.register(signal.SIGSEGV, all_threads=True)` to dump Python stack(s).
  * Set `ulimit -c unlimited` (or systemd `LimitCORE=`) to get core dumps; postmortem with `gdb` if needed.
  * Consider `PYTHONFAULTHANDLER=1` in prod env.
* **Reduce segfault risk at source**:

  * Pin known-good wheel builds; avoid mixing BLAS stacks (MKL/OpenBLAS) across libs.
  * Run CI with Address/UB Sanitizer against native deps (debug builds) and corpus fuzz a bit.
  * Cap native thread counts (`OMP_NUM_THREADS`, `MKL_NUM_THREADS`) to avoid oversubscription bugs.
* **Design for fast recovery**:

  * Checkpoint minimal state frequently (idempotent queues, last offsets, in-flight intents) so a crashed sidecar can resume cleanly.
  * Keep your orchestrator minimal and “pure Python” where possible; push native/jittery work out of process.

Bottom line: assume **process-level failure is inevitable** over a long enough horizon. Your safety net is **process boundaries + supervision + quick resume**, not `try/except`. Keep most minions in the monolith; split only the ones that touch risky native code or have memory spikes.
grf